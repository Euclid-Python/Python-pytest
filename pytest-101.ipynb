{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "\n",
    "current_notebook = 'pytest-101.ipynb'\n",
    "\n",
    "__file__ = current_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create test\n",
    "\n",
    "Now you know how to do... it's so simple.\n",
    "\n",
    "```python\n",
    "def test_something():\n",
    "    #blabla\n",
    "    assert something\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Customize the assertion message\n",
    "\n",
    "You could use the message feature of standard `assert`\n",
    "\n",
    "```python \n",
    "assert condition, message\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                        [100%]\n",
      "=================================== FAILURES ===================================\n",
      "________________________________ test_something ________________________________\n",
      "\n",
      "    def test_something():\n",
      "        # given\n",
      "        actual = 'A'\n",
      "        expected = 'B'\n",
      "    \n",
      ">       assert actual == expected, f'You mess the situation, {actual} is not {expected}'\n",
      "E       AssertionError: You mess the situation, A is not B\n",
      "E       assert 'A' == 'B'\n",
      "E         - A\n",
      "E         + B\n",
      "\n",
      "<ipython-input-2-94bb9a1d6309>:6: AssertionError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_something():\n",
    "    # given\n",
    "    actual = 'A'\n",
    "    expected = 'B'\n",
    "    \n",
    "    assert actual == expected, f'You mess the situation, {actual} is not {expected}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There's some more customization available **per argument type** see [official documentation](http://doc.pytest.org/en/latest/assert.html#making-use-of-context-sensitive-comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Check a exception  is raised\n",
    "\n",
    "_More difficult._\n",
    "\n",
    "Image the _Processing Under Test_ must raise an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_planet_name(name: str):\n",
    "    if name not in 'Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune'.split():\n",
    "        raise ValueError(f'{name} is not a planet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                        [100%]\n",
      "=================================== FAILURES ===================================\n",
      "_______________________________ test_is_a_planet _______________________________\n",
      "\n",
      "    def test_is_a_planet():\n",
      ">       check_planet_name('foo')\n",
      "\n",
      "<ipython-input-4-3528dc69e803>:5: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "name = 'foo'\n",
      "\n",
      "    def check_planet_name(name: str):\n",
      "        if name not in 'Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune'.split():\n",
      ">           raise ValueError(f'{name} is not a planet')\n",
      "E           ValueError: foo is not a planet\n",
      "\n",
      "<ipython-input-3-903e962f3fdb>:3: ValueError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet():\n",
    "    check_planet_name('Mercury')\n",
    "    \n",
    "def test_is_a_planet():\n",
    "    check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pytest uses a context manager `pytest.raises`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                        [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet():\n",
    "    with pytest.raises(ValueError):\n",
    "        check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                        [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "        \n",
    "def test_is_a_planet_with_exception_detail():\n",
    "    with pytest.raises(ValueError) as exception :\n",
    "        check_planet_name('foo')        \n",
    "    assert str(exception.value) == 'foo is not a planet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                        [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet_with_matcher():\n",
    "    with pytest.raises(ValueError, match=r'foo is*' ):\n",
    "        check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For more information, please take a look at [pytest Raises Official Documentation](https://docs.pytest.org/en/latest/reference.html#pytest-raises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixtures\n",
    "\n",
    "`Fixture` is a very powerful feature of pytest.\n",
    "\n",
    "It make easy \n",
    "\n",
    "* factorisation and composition of common processing\n",
    "* identification of setup and teardown steps\n",
    "* to distinguish heavy and expensive operation for integration tests from lighweight and cheap operation for unit tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's a fixture ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A factorized processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine a `Drone` class, powered by several `Engine`s and managed by a `DriverUnit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Drone:\n",
    "    \n",
    "    def __init__(self, engines, drive_unit):\n",
    "        self.engines = engines\n",
    "        self.drive_unit = drive_unit\n",
    "        \n",
    "        \n",
    "    def start(self):\n",
    "        for engine in self.engines:\n",
    "            engine.start()\n",
    "            \n",
    "    def is_started(self):\n",
    "        return all(engine.started for engine in self.engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To unit test the `Drone`  **in isolation of others object**, we create mock `Engine` and `DriveUnit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MockEngine:\n",
    "    def start(self):\n",
    "        self.status = 'started'\n",
    "        \n",
    "    def is_started(self):\n",
    "        return self.status == 'started'\n",
    "    \n",
    "class MockDriveUnit:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                        [100%]\n",
      "1 passed in 0.02s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -q\n",
    "\n",
    "def test_start_drone():\n",
    "    # given\n",
    "    engines = [MockEngine() for _ in range(1,4)]\n",
    "    drone= Drone(engines=engines,drive_unit=MockDriveUnit())\n",
    "    \n",
    "    # When\n",
    "    drone.start()\n",
    "    \n",
    "    #then\n",
    "    assert all(engine.status == 'started' for engine in engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's a little bit heavy to repeat no ?\n",
    "\n",
    "What is the fundamental law of any programer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>DRY</center>\n",
    "<div class=\"alert alert-info\">\n",
    "    <center>\n",
    "    <b>\n",
    "        <span style=\"font-size:larger;\">DON'T<br>REPEAT<br>YOURSELF</span>\n",
    "     </b>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Never,</center>\n",
    "<center>ever</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we can factorize the set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_drone():\n",
    "    engines = [MockEngine() for _ in range(1,4)]\n",
    "    drive_unit = MockDriveUnit()\n",
    "    drone= Drone(engines=engines,drive_unit=drive_unit)\n",
    "    return drone, engines, drive_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                        [100%]\n",
      "1 passed in 0.02s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -q\n",
    "\n",
    "def test_start_drone():\n",
    "    # given\n",
    "    drone, engines, drive_unit = init_drone()\n",
    "    \n",
    "    # When\n",
    "    drone.start()\n",
    "    \n",
    "    #then\n",
    "    assert all(engine.status == 'started' for engine in engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_Pytest_ proposes a fixture mechanism to standardise this kind of factorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Firstly**\n",
    "\n",
    "* Annotate a function with `@pytest.fixture` mark\n",
    "* Make the function create the right context and return object of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Secondly**\n",
    "\n",
    "* Use the ***name*** of the function as argument of the test.\n",
    "* Use the returned values from fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "uhm ?\n",
    "\n",
    "Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def init_drone():\n",
    "    engines = [MockEngine() for _ in range(1,4)]\n",
    "    drive_unit = MockDriveUnit()\n",
    "    drone= Drone(engines=engines,drive_unit=drive_unit)\n",
    "    return drone, engines, drive_unit    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                        [100%]\n",
      "1 passed in 0.02s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -q\n",
    "\n",
    "def test_start_drone(init_drone):\n",
    "    # given\n",
    "    drone, engines, drive_unit = init_drone\n",
    "    \n",
    "    # When\n",
    "    drone.start()\n",
    "    \n",
    "    #then\n",
    "    assert all(engine.status == 'started' for engine in engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_But it's the same, no ?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really, because it's a declarative way of injecting dependency.  \n",
    "For the sake of simplicity, we put the fixture near the test.\n",
    "\n",
    "But the _fixture_ could be shared among several tests and defined in other part of the project.\n",
    "\n",
    "Pytest will collect and inject them according to the name of the parameter.\n",
    "\n",
    "As user, we don't have to care **how** to inject, _Pytest_ does it for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixture composition\n",
    "\n",
    "What appends if the fixture needs another common function ?\n",
    "\n",
    "That's simple: it use the same mechanism :)\n",
    "\n",
    "`def <function_name>(<dependency_name>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                        [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "@pytest.fixture\n",
    "def a():\n",
    "    v = ['A']\n",
    "    return v\n",
    "\n",
    "@pytest.fixture\n",
    "def b(a):\n",
    "    a.append('B')\n",
    "    return a\n",
    "\n",
    "def test_ab(b):\n",
    "    assert b == ['A', 'B']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The _consummer_ can ask for as many dependencies as it need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                        [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_ab(a,b): # Use 2 dependencies\n",
    "    \n",
    "    first_list = a\n",
    "    second_list = b\n",
    "    \n",
    "    assert first_list == ['A','B']\n",
    "    assert second_list == ['A', 'B']"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
