{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "\n",
    "current_notebook = 'pytest-101.ipynb'\n",
    "\n",
    "__file__ = current_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create test\n",
    "\n",
    "Now you know how to do... it's so simple.\n",
    "\n",
    "```python\n",
    "def test_something():\n",
    "    #blabla\n",
    "    assert something\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Customize the assertion message\n",
    "\n",
    "You could use the message feature of standard `assert`\n",
    "\n",
    "```python \n",
    "assert condition, message\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                                                                [100%]\n",
      "====================================================== FAILURES =======================================================\n",
      "___________________________________________________ test_something ____________________________________________________\n",
      "\n",
      "    def test_something():\n",
      "        # given\n",
      "        actual = 'A'\n",
      "        expected = 'B'\n",
      "    \n",
      ">       assert actual == expected, f'You have messed, {actual} is not {expected}'\n",
      "E       AssertionError: You have messed, A is not B\n",
      "E       assert 'A' == 'B'\n",
      "E         - A\n",
      "E         + B\n",
      "\n",
      "<ipython-input-2-ec3693ef7c6d>:6: AssertionError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_something():\n",
    "    # given\n",
    "    actual = 'A'\n",
    "    expected = 'B'\n",
    "    \n",
    "    assert actual == expected, f'You have messed, {actual} is not {expected}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There's some more customization available **per argument type** see [official documentation](http://doc.pytest.org/en/latest/assert.html#making-use-of-context-sensitive-comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Check a exception  is raised\n",
    "\n",
    "_More difficult._\n",
    "\n",
    "Image the _Processing Under Test_ must raise an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_planet_name(name: str):\n",
    "    if name not in 'Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune'.split():\n",
    "        raise ValueError(f'{name} is not a planet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                                                                [100%]\n",
      "====================================================== FAILURES =======================================================\n",
      "__________________________________________________ test_is_a_planet ___________________________________________________\n",
      "\n",
      "    def test_is_a_planet():\n",
      ">       check_planet_name('foo')\n",
      "\n",
      "<ipython-input-4-3528dc69e803>:5: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "name = 'foo'\n",
      "\n",
      "    def check_planet_name(name: str):\n",
      "        if name not in 'Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune'.split():\n",
      ">           raise ValueError(f'{name} is not a planet')\n",
      "E           ValueError: foo is not a planet\n",
      "\n",
      "<ipython-input-3-903e962f3fdb>:3: ValueError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet():\n",
    "    check_planet_name('Mercury')\n",
    "    \n",
    "def test_is_a_planet():\n",
    "    check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pytest uses a context manager `pytest.raises`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simple one: just check if exception is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet():\n",
    "    with pytest.raises(ValueError):\n",
    "        check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Detailled one: get the raised exception and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "        \n",
    "def test_is_a_planet_with_exception_detail():\n",
    "    with pytest.raises(ValueError) as exception :\n",
    "        check_planet_name('foo')        \n",
    "    assert str(exception.value) == 'foo is not a planet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Matches the exception's message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet_with_matcher():\n",
    "    with pytest.raises(ValueError, match=r'foo is*' ):\n",
    "        check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For more information, please take a look at [pytest Raises Official Documentation](https://docs.pytest.org/en/latest/reference.html#pytest-raises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixtures\n",
    "\n",
    "`Fixture` is a very powerful feature of pytest.\n",
    "\n",
    "It make easy \n",
    "\n",
    "* factorisation and composition of common processing\n",
    "* identification of setup and teardown steps\n",
    "* to distinguish heavy and expensive operation for integration tests from lighweight and cheap operation for unit tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's a fixture ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A factorized processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine a `Drone` class, powered by several `Engine`s and managed by a `DriverUnit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Drone:\n",
    "    \n",
    "    def __init__(self, engines, drive_unit):\n",
    "        self.engines = engines\n",
    "        self.drive_unit = drive_unit\n",
    "        \n",
    "        \n",
    "    def start(self):\n",
    "        for engine in self.engines:\n",
    "            engine.start()\n",
    "            \n",
    "    def is_started(self):\n",
    "        return all(engine.started for engine in self.engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To unit test the `Drone`  **in isolation of others object**, we create mock `Engine` and `DriveUnit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MockEngine:\n",
    "    def start(self):\n",
    "        self.status = 'started'\n",
    "        \n",
    "    def is_started(self):\n",
    "        return self.status == 'started'\n",
    "    \n",
    "class MockDriveUnit:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n",
      "1 passed in 0.01s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -q\n",
    "\n",
    "def test_start_drone():\n",
    "    # given\n",
    "    engines = [MockEngine() for _ in range(1,4)]\n",
    "    drone= Drone(engines=engines,drive_unit=MockDriveUnit())\n",
    "    \n",
    "    # When\n",
    "    drone.start()\n",
    "    \n",
    "    #then\n",
    "    assert all(engine.status == 'started' for engine in engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's a little bit heavy to repeat no ?\n",
    "\n",
    "What is the fundamental law of any programer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>DRY</center>\n",
    "<div class=\"alert alert-info\">\n",
    "    <center>\n",
    "    <b>\n",
    "        <span style=\"font-size:larger;\">DON'T<br>REPEAT<br>YOURSELF</span>\n",
    "     </b>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Never,</center>\n",
    "<center>ever</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we can factorize the set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context():\n",
    "    engines = [MockEngine() for _ in range(1,4)]\n",
    "    drive_unit = MockDriveUnit()\n",
    "    drone= Drone(engines=engines,drive_unit=drive_unit)\n",
    "    return drone, engines, drive_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n",
      "1 passed in 0.01s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -q\n",
    "\n",
    "def test_start_drone():\n",
    "    # given\n",
    "    drone, engines, drive_unit = init_context()\n",
    "    \n",
    "    # When\n",
    "    drone.start()\n",
    "    \n",
    "    #then\n",
    "    assert all(engine.status == 'started' for engine in engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_Pytest_ proposes a fixture mechanism to standardise this kind of factorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Firstly**\n",
    "\n",
    "* Annotate a function with `@pytest.fixture` mark\n",
    "* Make the function create the right context and return object of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Secondly**\n",
    "\n",
    "* Use the ***name*** of the function as argument of the test.\n",
    "* Use the returned values from fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "uhm ?\n",
    "\n",
    "Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def init_context():\n",
    "    engines = [MockEngine() for _ in range(1,4)]\n",
    "    drive_unit = MockDriveUnit()\n",
    "    drone= Drone(engines=engines,drive_unit=drive_unit)\n",
    "    return drone, engines, drive_unit    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n",
      "1 passed in 0.01s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -q\n",
    "\n",
    "def test_start_drone(init_context):\n",
    "    # given\n",
    "    drone, engines, drive_unit = init_context\n",
    "    \n",
    "    # When\n",
    "    drone.start()\n",
    "    \n",
    "    #then\n",
    "    assert all(engine.status == 'started' for engine in engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_But it's the same, no ?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really, because it's a declarative way of injecting dependency.  \n",
    "For the sake of simplicity, we put the fixture near the test.\n",
    "\n",
    "But the _fixture_ could be \n",
    "* shared among several tests \n",
    "* and defined in other locations\n",
    "\n",
    "Pytest will collect and inject them according to the name of the parameter.\n",
    "\n",
    "As user, we don't have to care **how** to inject, _Pytest_ does it for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/pytest-fixture-injection.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixture composition\n",
    "\n",
    "What appends if the fixture needs another common function ?\n",
    "\n",
    "That's simple: it use the same mechanism :)\n",
    "\n",
    "`def <consummer_name>(<dependency_name>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def init_engines():\n",
    "    engines = [MockEngine() for _ in range(1,4)]\n",
    "    return engines\n",
    "\n",
    "@pytest.fixture\n",
    "def init_context(init_engines):\n",
    "    engines = init_engines\n",
    "    drive_unit = MockDriveUnit()\n",
    "    drone= Drone(engines=engines,drive_unit=drive_unit)\n",
    "    return drone, engines, drive_unit   \n",
    "\n",
    "def test_start_drone(init_context):\n",
    "    # given\n",
    "    drone, engines, drive_unit = init_context\n",
    "    # When\n",
    "    drone.start()\n",
    "    #then\n",
    "    assert all(engine.status == 'started' for engine in engines)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The _consummer_ can ask for as many dependencies as it need.\n",
    "\n",
    "`def <consummer_name>(<dependency_name_0>,<dependency_name_1>,...,<dependency_name_p>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def init_drive_unit():\n",
    "     return MockDriveUnit()\n",
    "\n",
    "@pytest.fixture\n",
    "def init_engines():\n",
    "    engines = [MockEngine() for _ in range(1,4)]\n",
    "    return engines\n",
    "\n",
    "@pytest.fixture\n",
    "def init_context(init_engines, init_drive_unit):\n",
    "    drone= Drone(engines=init_engines,drive_unit=init_drive_unit)\n",
    "    return drone, init_engines, init_drive_unit\n",
    "\n",
    "def test_start_drone(init_context):\n",
    "    # given\n",
    "    drone, engines, drive_unit = init_context\n",
    "    # When\n",
    "    drone.start()\n",
    "    #then\n",
    "    assert all(engine.status == 'started' for engine in engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixture setup and teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we need _setup_ and _teardown_ phases before and after **each** test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine we get a Computer adding numbers from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my.Computer import Computer\n",
    "\n",
    "class FileBasedComputer(Computer):\n",
    "  \n",
    "    def __init__(self,file):\n",
    "        super().__init__()\n",
    "        self.file = file\n",
    "    \n",
    "    def add_all(self):\n",
    "        with open(self.file,'r') as f:\n",
    "            self.add_collection(list(f.readlines()))\n",
    "            \n",
    "    def add_collection(self, collection):\n",
    "        for s in collection:\n",
    "            self.add(float(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "\n",
    "numbers = list(range(1,5))\n",
    "\n",
    "@pytest.fixture\n",
    "def init_file():\n",
    "    filename = 'test-temp.txt'\n",
    "    with open(filename,'w') as file:\n",
    "        file.write('\\n'.join([str(i) for i in numbers]))\n",
    "    return filename\n",
    "\n",
    "def test_add_collection(init_file):\n",
    "    # given\n",
    "    file_based_computer = FileBasedComputer(init_file)\n",
    "    # when\n",
    "    file_based_computer.add_all()\n",
    "    # then\n",
    "    assert file_based_computer.total == 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It works, but maybe do we want to teardown and clean everything after a test is done.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "import os\n",
    "\n",
    "\n",
    "numbers = list(range(1,5))\n",
    "\n",
    "@pytest.fixture\n",
    "def init_file():\n",
    "    filename = 'test-temp.txt'\n",
    "    with open(filename,'w') as file:\n",
    "        file.write('\\n'.join([str(i) for i in numbers]))\n",
    "        \n",
    "    yield filename\n",
    "    \n",
    "    os.remove(filename)\n",
    "    \n",
    "\n",
    "def test_add_collection(init_file):\n",
    "    # given\n",
    "    file_based_computer = FileBasedComputer(init_file)\n",
    "    # when\n",
    "    file_based_computer.add_all()\n",
    "    # then\n",
    "    assert file_based_computer.total == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/pytest-fixture-setup-teardown.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For ***Unit test*** it's not mandatory, but for ***Integration test*** or another heavy testing it's mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/pytest-heavy-setup-teardown.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some setup has to be done for the whole test suite, as creating Database, starting an application, locking some resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we can not create DB for each test; So we have to get different **scopes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/pytest-heavy-setup-teardown-different-scope.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some operations are performed for the whole test suite, but others have to be renewed at each test.\n",
    "\n",
    "They differ by **SCOPE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixture scoping\n",
    "\n",
    "_Pytest_ use several scopes, the main are\n",
    "* function,\n",
    "* module,\n",
    "* session\n",
    "\n",
    "For more details see [Scope official documentation](https://docs.pytest.org/en/latest/fixture.html#scope-sharing-a-fixture-instance-across-tests-in-a-class-module-or-session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Function scope\n",
    "\n",
    "* Fixture is ran at **each test**:\n",
    "    * setup \n",
    "    * yield &rarr; run function `test_xxx(...)`\n",
    "    * teardown "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/pytest-scope-function.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Module scope\n",
    "\n",
    "*  Fixture is ran at **each module** (file) :\n",
    "    * setup \n",
    "    * yield &rarr; run ***all*** functions `test_xxx(...)` in `test_nnn.py` file\n",
    "    * teardown "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/pytest-scope-module.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Session scope\n",
    "\n",
    "*  Fixture is ran **once** for the whole pytest session:\n",
    "    * setup \n",
    "    * yield &rarr; run ***all tests*** found by pytest\n",
    "    * teardown \n",
    "    \n",
    "It's very useful for setting up and teardown of heavy and expensive resources consumming tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"images/pytest-scope-session.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixture conftest\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
