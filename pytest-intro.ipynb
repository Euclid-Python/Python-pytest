{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest\n",
    "\n",
    "Pytest is a Unit Test framework for python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytest presentation\n",
    "\n",
    "\" _pytest is a mature full-featured Python testing tool that helps you write better programs._ \"  [according to documentation](https://docs.pytest.org/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytest philosophie is to provide a convenient way to write \n",
    "\n",
    "* from small unit tests\n",
    "* to full functional tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytest is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A library\n",
    "\n",
    "To use *pytest*, install it in your project\n",
    "\n",
    "```bash\n",
    "# pip\n",
    "pip install pytest\n",
    "# conda\n",
    "conda install -c anaconda pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A command\n",
    "\n",
    "_Pytest_ is mainly use as a **command**\n",
    "\n",
    "```bash\n",
    "$ pytest -v\n",
    "```\n",
    "\n",
    "but you can use it as a **module**\n",
    "\n",
    "```bash\n",
    "$ python -m pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## As a philosophy\n",
    "\n",
    "*Pytest* is committed to make your tests easy to design and run.\n",
    "\n",
    "* Very easy tests design\n",
    "* Discovery by convention\n",
    "* Easy lifecycle with fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using pytest the usual way\n",
    "\n",
    "- Creates a project _pytestwordir_  with a virtualenv inside \n",
    "\n",
    "`python -m venv pytestwordir/venv`\n",
    "\n",
    "- Go inside and activate the virtualenv\n",
    "    - Linux  &rarr; `. ./ven/bin/activate`\n",
    "    - Windows &rarr; `\\Scripts\\activate`\n",
    "- Install pytest \n",
    "\n",
    "`pip install pytest`\n",
    "\n",
    "- Creates a package _'my'_ \n",
    "\n",
    "`mkdir my`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Creates a file called `my/Computer.py` and copy paste following\n",
    "\n",
    "```python\n",
    "class Computer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total = 0\n",
    "        \n",
    "    def add(self,a):\n",
    "        self.total = self.total + a\n",
    "        return self.total\n",
    "    \n",
    "    def substract(self,a):\n",
    "        self.total = self.total - a\n",
    "        return self.total\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creates a unit test file `my/test_computer.py` and copy/past\n",
    "\n",
    "```python\n",
    "from my.Computer import Computer\n",
    "\n",
    "def test_simple_addition():\n",
    "    computer = Computer()\n",
    "    computer.add(1)\n",
    "    assert computer.total == 1\n",
    "\n",
    "def test_simple_substraction():\n",
    "    computer = Computer()\n",
    "    computer.substract(1)\n",
    "    assert computer.total == -1\n",
    "\n",
    "def test_nope():\n",
    "    computer = Computer()\n",
    "    assert computer.total == 0\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Run  the pytest command\n",
    "\n",
    "```bash\n",
    "$ pytest my/\n",
    "\n",
    "================================== test session starts ======================\n",
    "platform win32 -- Python 3.7.1, pytest-5.2.1, py-1.8.0, pluggy-0.13.0\n",
    "rootdir: C:\\temp\\foo\n",
    "collected 3 items\n",
    "\n",
    "my\\test_computer.py ...                                                [100%]\n",
    "\n",
    "================================== 3 passed in 0.03s ========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Add a new class to `my/Computer.py`\n",
    "\n",
    "```python\n",
    "class NumberWriter:\n",
    "\n",
    "    mapping = {'0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four', '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'}\n",
    "\n",
    "    def as_words(self,number):\n",
    "        \"\"\"Return a number decomposed as words separated by a -\"\"\"\n",
    "        number_as_str = str(number)\n",
    "        return '-'.join([NumberWriter.mapping[c] for c in number_as_str])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And new unit tests `my/test_writer.py`\n",
    "\n",
    "```python\n",
    "from my.Computer import NumberWriter\n",
    "\n",
    "def test_writer_one():\n",
    "    writer = NumberWriter()\n",
    "    assert writer.as_words(1) == 'one'\n",
    "\n",
    "def test_writer_several():\n",
    "    writer = NumberWriter()\n",
    "    assert writer.as_words(123) == 'one-two-three'    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run pytest with the verbose option `-v`\n",
    "\n",
    "```bash\n",
    "pytest m -v\n",
    "========================== test session starts ==========================\n",
    "platform win32 -- Python 3.7.1, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- c:\\temp\\foo\\scripts\\python.exe\n",
    "cachedir: .pytest_cache\n",
    "rootdir: C:\\temp\\foo\n",
    "collected 5 items\n",
    "\n",
    "my/test_computer.py::test_simple_addition PASSED                   [ 20%]\n",
    "my/test_computer.py::test_simple_substraction PASSED               [ 40%]\n",
    "my/test_computer.py::test_nope PASSED                              [ 60%]\n",
    "my/test_writer.py::test_writer_one PASSED                          [ 80%]\n",
    "my/test_writer.py::test_writer_several PASSED                      [100%]\n",
    "\n",
    "=========================== 5 passed in 0.03s ===========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, pytest \n",
    "* detect all tests, \n",
    "* run them for you \n",
    "* and display a report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytest discovery \n",
    "\n",
    "We describe here the default mechanism. Consult the [Official description](https://docs.pytest.org/en/latest/goodpractices.html#conventions-for-python-test-discovery) to get all variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Directory discovery\n",
    "\n",
    "* No argument &rarr; Starts from the current directory (*where the command is ran*)\n",
    "* Directory argument &rarr; Starts from the argument directory ( as `pytest my/` )\n",
    "* Runs **recursevely** through directory from **starting point**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## File discovery\n",
    "\n",
    "Search for \n",
    "\n",
    "* `test_*.py` \n",
    "* `*_test.py` \n",
    "\n",
    "and import them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tests discovery (outside of class)\n",
    "\n",
    "Only `test` **prefixed** test functions\n",
    "\n",
    "```python\n",
    "def test_something(): # <- OK\n",
    "    #...\n",
    "def something_test(): # <- not detected\n",
    "    #...\n",
    "def something(): # <- not detected\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tests discovery  (inside of class) \n",
    "\n",
    "Only `test` **prefixed** test functions or methods (the same)\n",
    "\n",
    "Class must \n",
    "* have `Test` prefixed class name (`TestComputer`)\n",
    "* have no `__init__` constructor\n",
    "\n",
    "```Python\n",
    "class TestSomething():\n",
    "    def test_something():\n",
    "        #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using pytest in a notebook\n",
    "\n",
    "We'll show some pytest into the notebook because it's more convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To make it working into notebook, we have to make some _magic_. ***It's not the recommended way of doing, it's only for the course.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**DISCLAIMER**\n",
    "\n",
    "A notebook is not a software. \n",
    "\n",
    "It's just a smart way of showing algorithmes, results, plots or courses. \n",
    "\n",
    "Please do not use notebook, also powerful they are, to code __software__.\n",
    "\n",
    "If you need to be convinced, see [Why I hate notebooks](https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g362da58057_0_1)\n",
    "</div>\n",
    "\n",
    "As powerpoint is not a painting tool, Notebook is not a software creating tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "The following instruction is to make pytest works in notebook with [ipytest library](https://pypi.org/project/ipytest/)\n",
    "    \n",
    "You don't have to do this in *normal* situation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "\n",
    "__file__ = 'pytest-intro.ipynb' # <- use the notebook filename to make ipytest scan python cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make pytest run on a notebook, **you have** to put this **magyc** notebook instruction ***at the beginning of the cell***.\n",
    "\n",
    "```python\n",
    "%%run_pytest[clean] -qq\n",
    "# Magyc instruction - ONLY FOR RUNNING PYTEST IN NOTEBOOK\n",
    "```\n",
    "\n",
    "* `-qq` to pass test silently\n",
    "* `-q` to have number of passed tests\n",
    "* without argument &rarr; the usual output\n",
    "* `-v` verbose mode\n",
    "* `[clean]` to make _ipytest_ forget the former tests in previous cells  \n",
    "and run only the current tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demonstration with our computer\n",
    "\n",
    "Let's include our class Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load my/Computer.py\n",
    "class Computer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total = 0\n",
    "        \n",
    "    def add(self,a):\n",
    "        self.total = self.total + a\n",
    "        return self.total\n",
    "    \n",
    "    def substract(self,a):\n",
    "        self.total = self.total - a\n",
    "        return self.total\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%run_pytest[clean]` not found.\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -v\n",
    "# Magyc instruction - ONLY FOR RUNNING PYTEST IN NOTEBOOK\n",
    "\n",
    "def test_addition():\n",
    "    # setup\n",
    "    computer = Computer()\n",
    "    # given\n",
    "    a = 1\n",
    "    # when\n",
    "    computer.add(a)\n",
    "    # then\n",
    "    assert computer.total == 1\n",
    "    # teardown\n",
    "    # nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytest 101\n",
    "\n",
    "Creates your first tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create test\n",
    "\n",
    "Now you know how to do... it's so simple.\n",
    "\n",
    "```python\n",
    "def test_something():\n",
    "    #blabla\n",
    "    assert something\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assertions\n",
    "\n",
    "Use the standard built-in assertion\n",
    "\n",
    "```python\n",
    "assert thing\n",
    "assert thing is None\n",
    "assert len(thing) > 100\n",
    "assert thing == True\n",
    "assert foo is not bar\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Customize the assertion message\n",
    "\n",
    "You could use the message feature of standard `assert`\n",
    "\n",
    "```python \n",
    "assert condition, message\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                                                                [100%]\n",
      "====================================================== FAILURES =======================================================\n",
      "___________________________________________________ test_something ____________________________________________________\n",
      "\n",
      "    def test_something():\n",
      "        # given\n",
      "        actual = 'A'\n",
      "        expected = 'B'\n",
      "    \n",
      ">       assert actual == expected, f'You have messed, {actual} is not {expected}'\n",
      "E       AssertionError: You have messed, A is not B\n",
      "E       assert 'A' == 'B'\n",
      "E         - A\n",
      "E         + B\n",
      "\n",
      "<ipython-input-2-ec3693ef7c6d>:6: AssertionError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_something():\n",
    "    # given\n",
    "    actual = 'A'\n",
    "    expected = 'B'\n",
    "    \n",
    "    assert actual == expected, f'You have messed, {actual} is not {expected}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There's some more customization available **per argument type** see [official documentation](http://doc.pytest.org/en/latest/assert.html#making-use-of-context-sensitive-comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Float comparison \n",
    "\n",
    "The good old problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Difference is 5.551115123125783e-17\nassert (0.1 + 0.2) == 0.3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Workdir/EUCLID/WorkshopPythonCpp/Python-pytest/pytest-intro.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Difference is {(0.1 + 0.2 - 0.3)}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Difference is 5.551115123125783e-17\nassert (0.1 + 0.2) == 0.3"
     ]
    }
   ],
   "source": [
    "assert 0.1 + 0.2 == 0.3, f'Difference is {(0.1 + 0.2 - 0.3)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pytest.approx\n",
    "\n",
    "Pytest offers a solution, [pytest.approx](https://docs.pytest.org/en/latest/reference.html#pytest-approx), to compare floats with a relative tolerance of 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0.1 + 0.2 == pytest.approx(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pytest import approx\n",
    "\n",
    "# scalar\n",
    "\n",
    "assert 0.1 + 0.2 == pytest.approx(0.3) # i.e. 0.3 ± 0.3*1e-6\n",
    "\n",
    "# sequence\n",
    "assert (0.1 + 0.2, 0.2 + 0.4) == approx((0.3, 0.6))\n",
    "\n",
    "# dict\n",
    "assert {'a': 0.1 + 0.2, 'b': 0.2 + 0.4} == approx({'a': 0.3, 'b': 0.6})\n",
    "\n",
    "# numpy\n",
    "assert np.array([0.1, 0.2]) + np.array([0.2, 0.4]) == approx(np.array([0.3, 0.6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Precision can be valued as **relative** or **absolute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5 ± 2.5e-02"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.approx(2.5, rel=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5 ± 1.0e-02"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.approx(2.5, abs=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Check an exception  is raised\n",
    "\n",
    "_More difficult._\n",
    "\n",
    "Image the _Processing Under Test_ must raise an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_planet_name(name: str):\n",
    "    if name not in 'Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune'.split():\n",
    "        raise ValueError(f'{name} is not a planet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                                                                [100%]\n",
      "====================================================== FAILURES =======================================================\n",
      "__________________________________________________ test_is_a_planet ___________________________________________________\n",
      "\n",
      "    def test_is_a_planet():\n",
      ">       check_planet_name('foo')\n",
      "\n",
      "<ipython-input-4-3528dc69e803>:5: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "name = 'foo'\n",
      "\n",
      "    def check_planet_name(name: str):\n",
      "        if name not in 'Mercury Venus Earth Mars Jupiter Saturn Uranus Neptune'.split():\n",
      ">           raise ValueError(f'{name} is not a planet')\n",
      "E           ValueError: foo is not a planet\n",
      "\n",
      "<ipython-input-3-903e962f3fdb>:3: ValueError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet():\n",
    "    check_planet_name('Mercury')\n",
    "    \n",
    "def test_is_a_planet():\n",
    "    check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pytest uses a context manager `pytest.raises` to check an exception is raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simple one: just check if exception is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                                                                                                                                                           [100%]\n",
      "==================================================================================================== FAILURES =====================================================================================================\n",
      "________________________________________________________________________________________________ test_is_a_planet _________________________________________________________________________________________________\n",
      "\n",
      "    def test_is_a_planet():\n",
      "        with pytest.raises(ValueError):\n",
      ">           check_planet_name('foo')\n",
      "E           NameError: name 'check_planet_name' is not defined\n",
      "\n",
      "<ipython-input-25-c68d3937e1d4>:3: NameError\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet():\n",
    "    with pytest.raises(ValueError):\n",
    "        check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Detailled one: get the raised exception and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "        \n",
    "def test_is_a_planet_with_exception_detail():\n",
    "    with pytest.raises(ValueError) as exception :\n",
    "        check_planet_name('foo')        \n",
    "    assert str(exception.value) == 'foo is not a planet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Matches the exception's message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                [100%]\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_is_a_planet_with_matcher():\n",
    "    with pytest.raises(ValueError, match=r'foo is*' ):\n",
    "        check_planet_name('foo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For more information, please take a look at [pytest Raises Official Documentation](https://docs.pytest.org/en/latest/reference.html#pytest-raises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Playing with files and directories\n",
    "\n",
    "When testing, you could need to use files or directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But as unit testing must **repeatable**, it must not depend on a specific location or computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You need <b><span style=\"font-size: larger\">temporary directories and files</span></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Temporary files fixtures\n",
    "\n",
    "You could do them *by hand* but *pytest* offers special functions to create temporary files and directory\n",
    "\n",
    "* `tmp_path` [source](https://github.com/pytest-dev/pytest/blob/master/src/_pytest/tmpdir.py#L164)\n",
    "* `tmp_path_factory`\n",
    "* `tmpdir`\n",
    "* `tmpdir_factory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main differences are about file abstraction and scope.\n",
    "\n",
    "* File System abstraction\n",
    "    * `tmp_path_*` &rarr; `pathlib/pathlib2.Path`\n",
    "    * `tmpdir_*` &rarr; `py.path.local` &rarr; `os.path`\n",
    "* Scope\n",
    "    * **simple type** (`tmp_path` and `tmpdir`) &rarr; each test (scope = function)\n",
    "    * **factories** &rarr; for the whole test session (scope = session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def count_words_with_characters(filename=None,*characters):\n",
    "    count=0\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            for word in line.split():\n",
    "                for character in characters:\n",
    "                    if character in word:\n",
    "                        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                                                                                                                                                                                                           [100%]\n",
      "1 passed in 0.03s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -q\n",
    "\n",
    "SPEECH=\"\"\"I have a dream that my four little children \n",
    "will one day live in a nation where they will not be judged \n",
    "by the color of their skin but by the content of their character.\"\"\"\n",
    "\n",
    "def test_count_words_with_characters(tmp_path):\n",
    "    # given\n",
    "    d = tmp_path / \"count_words_with_characters\"\n",
    "    d.mkdir()\n",
    "    p = d / \"test1.txt\"\n",
    "    p.write_text(SPEECH)\n",
    "    \n",
    "    assert count_words_with_characters(str(p),'a') == 8\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But where do these special functions come from  ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's discover the power of <a href=\"pytest-fixtures.ipynb\">fixtures</a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
